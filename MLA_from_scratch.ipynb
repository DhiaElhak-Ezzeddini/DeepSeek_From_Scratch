{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78861462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be03ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLA(nn.Module):\n",
    "    def __init__(self,d_model,n_heads,kv_latent_dim):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.kv_dim_model = kv_latent_dim\n",
    "        self.dh = d_model // n_heads ## dim per head\n",
    "        \n",
    "        ## Projection layers\n",
    "        self.W_q   = nn.Linear(d_model,d_model,bias=False)\n",
    "        self.W_dkv = nn.Linear(d_model,kv_latent_dim,bias=False)\n",
    "        self.W_uk  = nn.Linear(kv_latent_dim,d_model,bias=False)\n",
    "        self.W_uv  = nn.Linear(kv_latent_dim,d_model,bias=False)\n",
    "        self.W_o   = nn.Linear(d_model,d_model,bias=False)\n",
    "        \n",
    "        self.ln = nn.LayerNorm(kv_latent_dim)\n",
    "        self.register_buffer(\"absorbed_k\",None) ## holds W_q @ W_uk\n",
    "        \n",
    "    def forward(self,x,kv_cache=None,past_length=0):\n",
    "        b,s,d = x.size()\n",
    "        ## compute absorbed_k once : W_q @ W_uk \n",
    "        if self.absorbed_k is None : \n",
    "            absorbed = torch.matmul(self.W_q.weight,self.W_uk.weight) # (d,latent_dim)\n",
    "            self.absorbed_k = absorbed.view(self.n_heads,self.dh,-1)\n",
    "        ## compress x into latent dimension\n",
    "        new_C_kv = self.ln(self.W_dkv(x)) ## (b,s,latent_dim)\n",
    "        if kv_cache is None : \n",
    "            C_kv = new_C_kv\n",
    "        else : \n",
    "            C_kv = torch.cat([kv_cache,new_C_kv],dim=1) ## (b,s_total,latent_dim)\n",
    "        \n",
    "        s_total = C_kv.size(1)\n",
    "        ## compress V into d_model, split into heads\n",
    "        V = self.W_uv(C_kv)\n",
    "        V = V.view(b,s_total,self.n_heads,self.dh).transpose(1,2)\n",
    "        ## split the input token vector into n_heads\n",
    "        q = x.view(b,s,self.n_heads,self.dh)\n",
    "        \n",
    "        ## Attention scores : absorbed query * updated C_kv\n",
    "        attn_scores = torch.zeros(b,self.n_heads,s,s_total,device=x.device)\n",
    "        for  h in range(self.n_heads):\n",
    "            tmp = torch.matmul(q[:,:,h],self.absorbed_k[h])   ## absorbed query\n",
    "            attn_scores[:,h] = torch.bmm(tmp,C_kv.transpose(1,2))\n",
    "        ## Scaling / Causal mask\n",
    "        attn_scores = attn_scores / (self.dh**0.5)\n",
    "        mask = torch.tril(torch.ones((s,s_total),device=x.device),diagonal=past_length)\n",
    "        attn_scores = attn_scores.masked_fill(mask.view(1,1,s,s_total)==0,float(\"-inf\"))\n",
    "        \n",
    "        ## Softmax to get the weights\n",
    "        attn_weights = F.softmax(attn_scores,dim=-1)\n",
    "        \n",
    "        \n",
    "        out_heads = []\n",
    "        for i in range(self.n_heads):\n",
    "            context_h = torch.matmul(attn_weights[:,h], V[:,h])\n",
    "            out_heads.append(context_h)\n",
    "        \n",
    "        \n",
    "        out = torch.cat(out_heads,dim=-1)\n",
    "        return self.W_o(out), C_kv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b579d46",
   "metadata": {},
   "source": [
    "#### Speed test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad747a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([1, 5, 512]) , Cache: torch.Size([1, 5, 256])\n",
      "Memory : Standard: 80.0 KB , Latent: 20.0 KB\n"
     ]
    }
   ],
   "source": [
    "def speed_test():\n",
    "    model = MLA(d_model=512,n_heads=8,kv_latent_dim=256)\n",
    "    x = torch.randn(1,5,512)\n",
    "    out , cache = model(x)\n",
    "    print(f\"Output: {out.shape} , Cache: {cache.shape}\")\n",
    "    \n",
    "    ## Memory Comparison \n",
    "    std_size = 2*2*10*512*4/1024\n",
    "    latent_size = 2*10*256*4/1024\n",
    "    print(f\"Memory : Standard: {std_size} KB , Latent: {latent_size} KB\")\n",
    "\n",
    "if __name__ == \"__main__\" : \n",
    "    speed_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bfd594f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 : Initial Inpput\n",
      "Output shape : torch.Size([1, 5, 8])\n",
      "Cache shape : torch.Size([1, 5, 4])\n",
      "\n",
      "Step2 : Append 1 token\n",
      "Output shape : torch.Size([1, 1, 8])\n",
      "Cache shape : torch.Size([1, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "def cache_usage():\n",
    "    torch.manual_seed(0)\n",
    "    model = MLA(d_model=8,n_heads=2,kv_latent_dim=4)\n",
    "\n",
    "    ##  Initial Inpput  ##\n",
    "    x1 = torch.randn(1,5,8)\n",
    "    out1 , cache1 = model(x1)\n",
    "    print(\"Step1 : Initial Inpput\")\n",
    "    print(f\"Output shape : {out1.shape}\")\n",
    "    print(f\"Cache shape : {cache1.shape}\")\n",
    "\n",
    "    ##  Append 1 token  ##\n",
    "    x2 = torch.randn(1,1,8)\n",
    "    out2 , cache2 = model(x2,kv_cache=cache1,past_length=5)\n",
    "    print(\"\\nStep2 : Append 1 token\")\n",
    "    print(f\"Output shape : {out2.shape}\")\n",
    "    print(f\"Cache shape : {cache2.shape}\")\n",
    "    \n",
    "cache_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ebadc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 : Initial Input of 50 tokens --> cache shape : torch.Size([1, 50, 4])\n",
      "Step 1 : Added 1 token -> cache shape: torch.Size([1, 51, 4])\n",
      "Step 2 : Added 1 token -> cache shape: torch.Size([1, 52, 4])\n",
      "Step 3 : Added 1 token -> cache shape: torch.Size([1, 53, 4])\n",
      "Step 4 : Added 1 token -> cache shape: torch.Size([1, 54, 4])\n",
      "Step 5 : Added 1 token -> cache shape: torch.Size([1, 55, 4])\n"
     ]
    }
   ],
   "source": [
    "def demo_kv_cache_growth(n_init_tokens=5 , n_new_tokens=3):\n",
    "    torch.manual_seed(0)\n",
    "    model = MLA(d_model=8,n_heads=2,kv_latent_dim=4)\n",
    "    ##  Initial Inpput  ##\n",
    "    x = torch.randn(1,n_init_tokens,8)\n",
    "    out , cache = model(x)\n",
    "    print(f\"Step1 : Initial Input of {n_init_tokens} tokens --> cache shape : {cache.shape}\")\n",
    "\n",
    "    ##  Incrementally append new tokens one at a time ##\n",
    "    for i in range(1,n_new_tokens+1) : \n",
    "        new_token = torch.randn(1,1,8)\n",
    "        out , cache = model(new_token,kv_cache=cache,past_length=cache.shape[1])\n",
    "        print(f\"Step {i} : Added 1 token -> cache shape: {cache.shape}\")\n",
    "demo_kv_cache_growth(n_init_tokens=50 , n_new_tokens=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class MLA(nn.Module):\n",
    "    def __init__(self,d_model,n_heads,kv_latent_dim):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.dh = d_model // n_heads ## dim per head\n",
    "        \n",
    "        ## Projection layers\n",
    "        self.W_q   = nn.Linear(d_model,d_model,bias=False)\n",
    "        self.W_uk  = nn.Linear(kv_latent_dim,d_model,bias=False)\n",
    "        self.W_uv  = nn.Linear(kv_latent_dim,d_model,bias=False)\n",
    "     \n",
    "model = MLA()\n",
    "\n",
    "## extract weights \n",
    "W_q = model.W_q.weight.detach().numpy()\n",
    "W_uk = model.W_uk.weight.detach().numpy()\n",
    "W_uv = model.W_uv.weight.detach().numpy()\n",
    "\n",
    "## split per head\n",
    "n_heads = model.n_heads\n",
    "dh = model.dh\n",
    "W_q_heads = W_q.reshape(n_heads,dh,-1)\n",
    "W_uk_heads = W_uk.reshape(n_heads,dh,-1)\n",
    "W_uv_heads = W_uv.reshape(n_heads,dh,-1)\n",
    "\n",
    "## Plot heatmaps\n",
    "fig , axes = plt.subplots() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
