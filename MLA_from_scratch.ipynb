{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78861462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be03ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLA(nn.Module):\n",
    "    def __init__(self,d_model,n_heads,kv_latent_dim):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.kv_dim_model = kv_latent_dim\n",
    "        self.dh = d_model // n_heads ## dim per head\n",
    "        \n",
    "        ## Projection layers\n",
    "        self.W_q   = nn.Linear(d_model,d_model,bias=False)\n",
    "        self.W_dkv = nn.Linear(d_model,kv_latent_dim,bias=False)\n",
    "        self.W_uk  = nn.Linear(kv_latent_dim,d_model,bias=False)\n",
    "        self.W_uv  = nn.Linear(kv_latent_dim,d_model,bias=False)\n",
    "        self.W_o   = nn.Linear(d_model,d_model,bias=False)\n",
    "        \n",
    "        self.ln = nn.LayerNorm(kv_latent_dim)\n",
    "        self.register_buffer(\"absorbed_k\",None) ## holds W_q @ W_uk\n",
    "        \n",
    "    def forward(self,x,kv_cache=None,past_length=0):\n",
    "        b,s,d = x.size()\n",
    "        ## compute absorbed_k once : W_q @ W_uk \n",
    "        if self.absorbed_k is None : \n",
    "            absorbed = torch.matmul(self.W_q.weight,self.W_uk.weight) # (d,latent_dim)\n",
    "            self.absorbed_k = absorbed.view(self.n_heads,self.dh,-1)\n",
    "        ## compress x into latent dimension\n",
    "        new_C_kv = self.ln(self.W_dkv(x)) ## (b,s,latent_dim)\n",
    "        if kv_cache is None : \n",
    "            C_kv = new_C_kv\n",
    "        else : \n",
    "            C_kv = torch.cat([kv_cache,new_C_kv],dim=1) ## (b,s_total,latent_dim)\n",
    "        \n",
    "        s_total = C_kv.size(1)\n",
    "        ## compress V into d_model, split into heads\n",
    "        V = self.W_uv(C_kv)\n",
    "        V.view(b,s_total,self.n_heads,self.dh).transpose(1,2)\n",
    "        ## split the input token vector into n_heads\n",
    "        q = x.view(b,s,self.n_heads,self.dh)\n",
    "        \n",
    "        ## Attention scores : absorbed query * updated C_kv\n",
    "        attn_scores = torch.zeros(b,self.n_heads,s,s_total,device=x.device)\n",
    "        for  h in range(self.n_heads):\n",
    "            tmp = torch.matmul(q[:,:,h],self.absorbed_k[h])   ## absorbed query\n",
    "            attn_scores[:,h] = torch.bmm(tmp,C_kv.transpose(1,2))\n",
    "        ## Scaling / Causal mask\n",
    "        attn_scores = attn_scores / (self.dh**0.5)\n",
    "        mask = torch.tril(torch.ones((s,s_total),device=x.device),diagonal=past_length)\n",
    "        attn_scores = attn_scores.masked_fill(mask.view(1,1,s,s_total)==0,float(\"-inf\"))\n",
    "        \n",
    "        ## Softmax to get the weights\n",
    "        attn_weights = F.softmax(attn_scores,dim=-1)\n",
    "        \n",
    "        \n",
    "        out_heads = []\n",
    "        for i in range(self.n_heads):\n",
    "            context_h = torch.matmul(attn_weights[:,h], V[:,h])\n",
    "            out_heads.append(context_h)\n",
    "        \n",
    "        \n",
    "        out = torch.cat(out_heads,dim=-1)\n",
    "        return self.W_o(out), C_kv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
